import math
import numpy as np
import scipy as sp
import scipy.linalg
import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F


# supported non-linearities: note that the function must be invertible
functional_derivatives = {
    torch.tanh: lambda x: 1 - torch.pow(torch.tanh(x), 2),
    F.leaky_relu: lambda x: (x > 0).type(torch.FloatTensor) + \
                            (x < 0).type(torch.FloatTensor) * -0.01,
    F.elu: lambda x: (x > 0).type(torch.FloatTensor) + \
                     (x < 0).type(torch.FloatTensor) * torch.exp(x)
}






class Planar(nn.Module):
     """
     Planar flow.

         z = f(x) = x + u h(wáµ€x + b)

     [Rezende and Mohamed, 2015]
     """
     def __init__(self, dim, nonlinearity=torch.tanh):
     # def __init__(self, dim, nonlinearity=F.leaky_relu):
         super().__init__()
         self.h = nonlinearity
         self.w = nn.Parameter(torch.Tensor(dim))
         self.u = nn.Parameter(torch.Tensor(dim))
         self.b = nn.Parameter(torch.Tensor(1))
         self.reset_parameters(dim)

     def reset_parameters(self, dim):
         init.uniform_(self.w, -math.sqrt(1/dim), math.sqrt(1/dim))
         init.uniform_(self.u, -math.sqrt(1/dim), math.sqrt(1/dim))
         init.uniform_(self.b, -math.sqrt(1/dim), math.sqrt(1/dim))

     def forward(self, x):
         """
         Given x, returns z and the log-determinant log|df/dx|.

         Returns
         -------
         """
         if self.h in (F.elu, F.leaky_relu):
             u = self.u
         elif self.h == torch.tanh:
             #scal = torch.log(1+torch.exp(self.w @ self.u)) - self.w @ self.u - 1
             scal = (self.w @ self.u) * torch.log(1/torch.exp(self.w @ self.u)+1) - self.w @ self.u - 1
             u = self.u + scal * self.w / torch.norm(self.w)
         else:
             raise NotImplementedError("Non-linearity is not supported.")
         lin = torch.unsqueeze(x @ self.w, 1) + self.b
         z = x + u * self.h(lin)
         phi = functional_derivatives[self.h](lin) * self.w
         #log_det = torch.log(torch.abs(1 + phi @ u) + 1e-4)
         log_det = torch.log(torch.abs(1 + phi @ u))
         return z, log_det

     def backward(self, z):
         raise NotImplementedError("Planar flow has no algebraic inverse.")
